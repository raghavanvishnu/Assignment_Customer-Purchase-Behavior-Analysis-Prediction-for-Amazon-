# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bQnY4pHz84yXd6JLtAjJHk3l3OizRCu6
"""

import pandas as pd
import numpy as np
from scipy.stats import zscore
!pip install feature_engine
from feature_engine.outliers import Winsorizer

# ------------------------------
# LOAD DATA
# ------------------------------
df = pd.read_excel("/content/Amazon_Customer_Purchase_Data.xlsx")

# ------------------------------
# MISSING VALUE HANDLING
# ------------------------------
num_cols = ["Age", "Purchase_Amount", "Rating", "Customer_Lifetime_Value", "Loyalty_Score"]

for col in num_cols:
    if col in df.columns:
        df[col].fillna(df[col].median(), inplace=True)

cat_cols = ["Gender", "Payment_Method", "Discount_Applied", "Return_Status",
            "Customer_Segment", "Preferred_Shopping_Channel"]

for col in cat_cols:
    if col in df.columns:
        df[col].fillna(df[col].mode()[0], inplace=True)

# -------------------------------------------------------
# REMOVE DUPLICATES
# -------------------------------------------------------
df.drop_duplicates(subset=["Customer_ID", "Purchase_Date"], inplace=True)

# -------------------------------------------------------
# DATA TYPE CORRECTION
# -------------------------------------------------------

# Convert date column
df["Purchase_Date"] = pd.to_datetime(df["Purchase_Date"], errors="coerce")

# Standardize Gender
df["Gender"] = df["Gender"].str.title().replace({
    "M": "Male", "F": "Female", "O": "Other"
})

# Convert to category dtypes
for col in cat_cols:
    if col in df.columns:
        df[col] = df[col].astype("category")

# Numeric type corrections
df["Age"] = pd.to_numeric(df["Age"], errors="coerce")
df["Purchase_Amount"] = pd.to_numeric(df["Purchase_Amount"], errors="coerce")
df["Rating"] = pd.to_numeric(df["Rating"], errors="coerce")

# -------------------------------------------------------
# OUTLIER HANDLING USING WINSORIZATION
# -------------------------------------------------------

winsor = Winsorizer(
    capping_method='iqr',
    tail='both',
    fold=1.5,
    variables=["Purchase_Amount", "Customer_Lifetime_Value"]
)
df = winsor.fit_transform(df)

# -------------------------------------------------------
# FEATURE ENGINEERING
# -------------------------------------------------------

# 1. CLV (recalculate to ensure consistency)
df["CLV_Recalc"] = df.groupby("Customer_ID")["Purchase_Amount"].transform("sum")

# 2. Loyalty Score (frequency + spending)
df["Total_Spent"] = df.groupby("Customer_ID")["Purchase_Amount"].transform("sum")
df["Purchase_Frequency"] = df.groupby("Customer_ID")["Purchase_Amount"].transform("count")

df["Loyalty_Score"] = (0.7 * df["Total_Spent"]) + (0.3 * df["Purchase_Frequency"])

# 3. Discount Applied (if already Yes/No keep as is)
df["Discount_Applied"] = df["Discount_Applied"].astype("category")

# 4. Return Status
df["Return_Status"] = df["Return_Status"].astype("category")

# 5. Customer Segment based on loyalty score
def segment(score):
    if score < df["Loyalty_Score"].quantile(0.33):
        return "New"
    elif score < df["Loyalty_Score"].quantile(0.66):
        return "Regular"
    else:
        return "VIP"

df["Customer_Segment"] = df["Loyalty_Score"].apply(segment).astype("category")

# 6. Preferred Shopping Channel
df["Preferred_Shopping_Channel"] = df["Preferred_Shopping_Channel"].str.title().astype("category")

# -------------------------------------------------------
# SAVE CLEANED DATA
# -------------------------------------------------------
df.to_csv("amazon_cleaned.csv", index=False)
df.head()

"""#K MEANS"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Load cleaned data
df = pd.read_csv("amazon_cleaned.csv")

# -------------------------------
# Select clustering features
# -------------------------------
cluster_features = df[["Total_Spent", "Purchase_Frequency", "Loyalty_Score"]]

# Scale data
scaler = StandardScaler()
scaled_features = scaler.fit_transform(cluster_features)

# -------------------------------
# Find optimal K (Elbow Method)
# -------------------------------
wcss = []
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 10), wcss, marker='o')
plt.title("Elbow Curve to Determine Optimal K")
plt.xlabel("Number of Clusters")
plt.ylabel("WCSS")
plt.show()

# -------------------------------
# Train Final K-Means Model
# -------------------------------
kmeans = KMeans(n_clusters=3, random_state=42)
df["Customer_Cluster"] = kmeans.fit_predict(scaled_features)

# Analyze clusters
df.groupby("Customer_Cluster")[["Total_Spent", "Purchase_Frequency", "Loyalty_Score"]].mean()

"""#Linear Regression"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# -------------------------------
# Prepare Features & Target
# -------------------------------

X = df[["Age", "Total_Spent", "Purchase_Frequency", "Loyalty_Score",
        "Discount_Applied", "Payment_Method"]]
y = df["Customer_Lifetime_Value"]

# Handle categorical variables
ct = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop='first'), ["Discount_Applied", "Payment_Method"])
    ],
    remainder="passthrough"
)

X_encoded = ct.fit_transform(X)

# -------------------------------
# Train-Test Split
# -------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.2, random_state=42
)

# -------------------------------
# Linear Regression Model
# -------------------------------
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# -------------------------------
# Model Evaluation
# -------------------------------
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("RMSE:", rmse)
print("RÂ² Score:", r2)

"""#Logistic Regression
#Create Churn Label
"""

# Create a churn label based on low loyalty or low frequency
df["Churn"] = df.apply(
    lambda x: 1 if (x["Loyalty_Score"] < df["Loyalty_Score"].quantile(0.30) and
                    x["Purchase_Frequency"] <= 1) else 0,
    axis=1
)

"""#Prepare Data for Logistic Regression

"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report

# Define features & target
X = df[[
    "Age",
    "Total_Spent",
    "Purchase_Frequency",
    "Loyalty_Score",
    "Discount_Applied",
    "Payment_Method",
    "Preferred_Shopping_Channel"
]]

y = df["Churn"]

# One-hot encode categorical variables
ct = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first"),
         ["Discount_Applied", "Payment_Method", "Preferred_Shopping_Channel"])
    ],
    remainder="passthrough"
)

X_encoded = ct.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.2, random_state=42
)

"""#Train Logistic Regression Model"""

model_churn = LogisticRegression(max_iter=1000)
model_churn.fit(X_train, y_train)

y_pred = model_churn.predict(X_test)

"""#Evaluate Churn Model"""

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("Accuracy:", acc)
print("Precision:", prec)
print("Recall:", rec)
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""# ----------------------------------------
# CREATE CUSTOMER-LEVEL FIELDS
# ----------------------------------------

# Total Spent per customer
df["Total_Spent"] = df.groupby("Customer_ID")["Purchase_Amount"].transform("sum")

# Purchase Frequency per customer
df["Purchase_Frequency"] = df.groupby("Customer_ID")["Purchase_Amount"].transform("count")

# Loyalty Score
df["Loyalty_Score"] = (0.7 * df["Total_Spent"]) + (0.3 * df["Purchase_Frequency"])

# CLV (your project formula)
df["CLV"] = df["Total_Spent"] * 0.5 + df["Purchase_Frequency"] * 10
# (Replace with your preferred method if needed)

# If Customer_Cluster already created with KMeans
# Ensure it exists
# df["Customer_Cluster"] should already be generated earlier

# Churn label (rule-based or from Logistic Regression)
df["Churn"] = df.apply(
    lambda x: 1 if (x["Loyalty_Score"] < df["Loyalty_Score"].quantile(0.30) and
                    x["Purchase_Frequency"] <= 1)
              else 0,
    axis=1
)

# ----------------------------------------
# SAVE UPDATED DATASET
# ----------------------------------------
df.to_csv("amazon_cleaned.csv", index=False)

df.head()

"""

# ----------------------------------------
# CREATE CUSTOMER-LEVEL FIELDS
# ----------------------------------------

# Total Spent per customer
df["Total_Spent"] = df.groupby("Customer_ID")["Purchase_Amount"].transform("sum")

# Purchase Frequency per customer
df["Purchase_Frequency"] = df.groupby("Customer_ID")["Purchase_Amount"].transform("count")

# Loyalty Score
df["Loyalty_Score"] = (0.7 * df["Total_Spent"]) + (0.3 * df["Purchase_Frequency"])

# CLV (your project formula)
df["CLV"] = df["Total_Spent"] * 0.5 + df["Purchase_Frequency"] * 10
# (Replace with your preferred method if needed)

# If Customer_Cluster already created with KMeans
# Ensure it exists
# df["Customer_Cluster"] should already be generated earlier

# Churn label (rule-based or from Logistic Regression)
df["Churn"] = df.apply(
    lambda x: 1 if (x["Loyalty_Score"] < df["Loyalty_Score"].quantile(0.30) and
                    x["Purchase_Frequency"] <= 1)
              else 0,
    axis=1
)

# ----------------------------------------
# SAVE UPDATED DATASET
# ----------------------------------------
df.to_csv("amazon_cleaned.csv", index=False)

df.head()